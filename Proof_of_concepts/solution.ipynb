{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a08a2fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load environment variables and validate required secrets.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    " \n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY or not OPENAI_API_KEY.strip():\n",
    "    raise EnvironmentError(\n",
    "        \"OPENAI_API_KEY not found. Please set it in the .env file.\"\n",
    "    )\n",
    "\n",
    "print(\"Environment variables loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4598e59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'llm': {'provider': 'openai', 'chat_model': 'gpt-4o-mini'},\n",
       " 'rag': {'corpus_path': '../data/AI Engineering.pdf',\n",
       "  'chunk_size': 512,\n",
       "  'chunk_overlap': 64,\n",
       "  'top_k': 5},\n",
       " 'vector_store': {'type': 'qdrant',\n",
       "  'mode': 'local',\n",
       "  'url': 'http://localhost:6333',\n",
       "  'collection_name': 'knowledge_base_chunks',\n",
       "  'storage_path': './qdrant_storage'},\n",
       " 'dense_vector': {'name': 'dense',\n",
       "  'model': 'sentence-transformers/all-MiniLM-L6-v2',\n",
       "  'distance': 'cosine'},\n",
       " 'sparse_vector': {'enabled': True, 'name': 'sparse', 'model': 'Qdrant/bm25'}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load and validate project configuration from parameters.toml.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import tomllib\n",
    "\n",
    "CONFIG_PATH = Path(\"parameters.toml\")\n",
    "\n",
    "if not CONFIG_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"parameters.toml not found. Please ensure it exists at project root.\"\n",
    "    )\n",
    "\n",
    "with open(CONFIG_PATH, \"rb\") as f:\n",
    "    config = tomllib.load(f)\n",
    "\n",
    "print(\"Configuration loaded successfully.\")\n",
    "config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56cb103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "class PDFDocumentLoader:\n",
    "    \"\"\"\n",
    "    Responsible for:\n",
    "    1. Opening a PDF safely\n",
    "    2. Counting pages\n",
    "    3. Extracting page-wise text with metadata\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pdf_path: str):\n",
    "        self.pdf_path = Path(pdf_path)\n",
    "\n",
    "        if not self.pdf_path.exists():\n",
    "            raise FileNotFoundError(f\"PDF not found at path: {self.pdf_path}\")\n",
    "\n",
    "    def get_total_pages(self) -> int:\n",
    "        \"\"\"Return total number of pages in the PDF\"\"\"\n",
    "        with fitz.open(self.pdf_path) as pdf_doc:\n",
    "            return pdf_doc.page_count\n",
    "\n",
    "    def extract_documents(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract text page-wise and return documents list\n",
    "        Each item is RAG-ready (text + metadata)\n",
    "        \"\"\"\n",
    "        documents = []\n",
    "\n",
    "        with fitz.open(self.pdf_path) as pdf_doc:\n",
    "            total_pages = pdf_doc.page_count\n",
    "\n",
    "            for page_index in range(total_pages):\n",
    "                page = pdf_doc.load_page(page_index)\n",
    "                text = page.get_text(\"text\")\n",
    "\n",
    "                # Skip empty / non-text pages\n",
    "                if not text or not text.strip():\n",
    "                    continue\n",
    "\n",
    "                documents.append(\n",
    "                    {\n",
    "                        \"text\": text.strip(),\n",
    "                        \"metadata\": {\n",
    "                            \"source\": self.pdf_path.name,\n",
    "                            \"page\": page_index + 1,\n",
    "                            \"char_count\": len(text)\n",
    "                        }\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa9b1e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages in PDF: 535\n",
      "Extracted text from 529 pages\n"
     ]
    }
   ],
   "source": [
    "loader = PDFDocumentLoader(config[\"rag\"][\"corpus_path\"])\n",
    "\n",
    "total_pages = loader.get_total_pages()\n",
    "print(f\"Total pages in PDF: {total_pages}\")\n",
    "\n",
    "documents = loader.extract_documents()\n",
    "print(f\"Extracted text from {len(documents)} pages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ef247b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "class DocumentInspector:\n",
    "    \"\"\"\n",
    "    Utility class to inspect extracted documents\n",
    "    before chunking and embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, documents: List[Dict]):\n",
    "        if not documents:\n",
    "            raise ValueError(\"No documents provided for inspection.\")\n",
    "        self.documents = documents\n",
    "\n",
    "    def preview(\n",
    "        self,\n",
    "        sample_size: int = 3,\n",
    "        max_chars: int = 500\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Print a small sample of extracted text with metadata.\n",
    "\n",
    "        :param sample_size: Number of documents to inspect\n",
    "        :param max_chars: Max characters to print per document\n",
    "        \"\"\"\n",
    "        print(f\"\\nInspecting {min(sample_size, len(self.documents))} document(s):\\n\")\n",
    "\n",
    "        for idx, doc in enumerate(self.documents[:sample_size], start=1):\n",
    "            text_preview = doc[\"text\"][:max_chars]\n",
    "\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"Sample #{idx}\")\n",
    "            print(f\"Source : {doc['metadata'].get('source')}\")\n",
    "            print(f\"Page   : {doc['metadata'].get('page')}\")\n",
    "            print(f\"Chars  : {doc['metadata'].get('char_count')}\")\n",
    "            print(\"-\" * 80)\n",
    "            print(text_preview)\n",
    "            print(\"...\" if len(doc[\"text\"]) > max_chars else \"\")\n",
    "            print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "275197e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inspecting 2 document(s):\n",
      "\n",
      "================================================================================\n",
      "Sample #1\n",
      "Source : AI Engineering.pdf\n",
      "Page   : 1\n",
      "Chars  : 73\n",
      "--------------------------------------------------------------------------------\n",
      "Chip Huyen\n",
      " AI Engineering\n",
      "Building Applications \n",
      "with Foundation Models\n",
      "\n",
      "================================================================================\n",
      "================================================================================\n",
      "Sample #2\n",
      "Source : AI Engineering.pdf\n",
      "Page   : 2\n",
      "Chars  : 2340\n",
      "--------------------------------------------------------------------------------\n",
      "9\n",
      "7 8 1 0 9 8 1 6 6 3 0 4\n",
      "5 7 9 9 9\n",
      "ISBN:   978-1-098-16630-4\n",
      "US $79.99\t   CAN $99.99\n",
      "DATA\n",
      "Foundation models have enabled many new AI use cases while lowering the barriers to entry for \n",
      "building AI products. This has transformed AI from an esoteric discipline into a powerful development \n",
      "tool that anyone can useâ€”including those with no prior AI experience.\n",
      "In this accessible guide, author Chip Huy\n",
      "...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "inspector = DocumentInspector(documents)\n",
    "inspector.preview(sample_size=2, max_chars=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "814b5a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "class TextNormalizer:\n",
    "    \"\"\"\n",
    "    Normalize raw PDF text to improve downstream\n",
    "    chunking and retrieval quality.\n",
    "    \"\"\"\n",
    "\n",
    "    _MULTIPLE_NEWLINES = re.compile(r\"\\n{3,}\")\n",
    "    _MULTIPLE_SPACES = re.compile(r\"[ \\t]{2,}\")\n",
    "    _SPACE_BEFORE_NEWLINE = re.compile(r\"[ \\t]+\\n\")\n",
    "    _LINE_WRAP = re.compile(r\"(?<!\\n)\\n(?!\\n)\")\n",
    "\n",
    "    def normalize_text(self, text: str) -> str:\n",
    "        if not text:\n",
    "            return text\n",
    "\n",
    "        # Normalize line endings\n",
    "        text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "        # Remove trailing spaces before newlines\n",
    "        text = self._SPACE_BEFORE_NEWLINE.sub(\"\\n\", text)\n",
    "\n",
    "        # Remove line-wrapped newlines (inside paragraphs)\n",
    "        text = self._LINE_WRAP.sub(\" \", text)\n",
    "\n",
    "        # Collapse excessive newlines (keep paragraphs)\n",
    "        text = self._MULTIPLE_NEWLINES.sub(\"\\n\\n\", text)\n",
    "\n",
    "        # Collapse repeated spaces\n",
    "        text = self._MULTIPLE_SPACES.sub(\" \", text)\n",
    "\n",
    "        return text.strip()\n",
    "\n",
    "    def normalize_documents(self, documents: List[Dict]) -> List[Dict]:\n",
    "        return [\n",
    "            {\n",
    "                \"text\": self.normalize_text(doc[\"text\"]),\n",
    "                \"metadata\": doc[\"metadata\"]\n",
    "            }\n",
    "            for doc in documents\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36fe9f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '9 7 8 1 0 9 8 1 6 6 3 0 4 5 7 9 9 9 ISBN: 978-1-098-16630-4 US $79.99 CAN $99.99 DATA Foundation models have enabled many new AI use cases while lowering the barriers to entry for building AI products. This has transformed AI from an esoteric discipline into a powerful development tool that anyone can useâ€”including those with no prior AI experience. In this accessible guide, author Chip Huyen discusses AI engineering: the process of building applications with readily available foundation models. AI application developers will discover how to navigate the AI landscape, including models, datasets, evaluation benchmarks, and the seemingly infinite number of application patterns. The book also introduces a practical framework for developing an AI application and efficiently deploying it. â€¢ Understand what AI engineering is and how it differs from traditional machine learning engineering â€¢ Learn the process for developing an AI application, the challenges at each step, and approaches to address them â€¢ Explore various model adaptation techniques, including prompt engineering, RAG, finetuning, agents, and dataset engineering, and understand how and why they work â€¢ Examine the bottlenecks for latency and cost when serving foundation models and learn how to overcome them â€¢ Choose the right model, metrics, data, and developmental patterns for your needs AI Engineering â€œThis book of fers a comprehensive, well-structured guide to the essential aspects of building generative AI systems. A must-read for any professional looking to scale AI across the enterprise.â€ Vittorio Cretella, former global CIO at P&G and Mars â€œChip Huyen gets generative AI. She is a remarkable teacher and writer whose work has been instrumental in helping teams bring AI into production. Drawing on her deep expertise, AI Engineering is a comprehensive and holistic guide to building generative AI applications in production.â€ Luke Metz, cocreator of ChatGPT, former research manager at OpenAI Chip Huyen works at the intersection of AI, data, and storytelling. Previously, she was with Snorkel AI and NVIDIA, founded an AI infrastructure startup (acquired), and taught machine learning systems design at Stanford. Her book Designing Machine Learning Systems (Oâ€™Reilly) has been translated into over 10 languages.',\n",
       " 'metadata': {'source': 'AI Engineering.pdf', 'page': 2, 'char_count': 2340}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer = TextNormalizer()\n",
    "normalized_documents = normalizer.normalize_documents(documents)\n",
    "normalized_documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cda32904",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentFilter:\n",
    "    \"\"\"\n",
    "    Filters out low-value or noisy pages\n",
    "    before chunking and embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, min_char_count: int = 200):\n",
    "        self.min_char_count = min_char_count\n",
    "\n",
    "    def is_useful(self, doc: dict) -> bool:\n",
    "        text = doc[\"text\"].lower()\n",
    "\n",
    "        # Filter very small pages\n",
    "        if doc[\"metadata\"][\"char_count\"] < self.min_char_count:\n",
    "            return False\n",
    "\n",
    "        # Filter common front-matter patterns\n",
    "        noise_markers = [\n",
    "            \"isbn\",\n",
    "            \"copyright\",\n",
    "            \"all rights reserved\",\n",
    "            \"table of contents\",\n",
    "            \"price\",\n",
    "            \"publisher\"\n",
    "        ]\n",
    "\n",
    "        if any(marker in text for marker in noise_markers):\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def filter_documents(self, documents: list) -> list:\n",
    "        return [doc for doc in documents if self.is_useful(doc)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d514e6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering: 529\n",
      "After filtering : 484\n"
     ]
    }
   ],
   "source": [
    "filterer = DocumentFilter(min_char_count=200)\n",
    "clean_documents = filterer.filter_documents(normalized_documents)\n",
    "\n",
    "print(f\"Before filtering: {len(normalized_documents)}\")\n",
    "print(f\"After filtering : {len(clean_documents)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d08432a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'This is the definitive segue into AI engineering from one of the greats of ML engineering! Chip has seen through successful projects and careers at every stage of a company and for the first time ever condensed her expertise for new AI Engineers entering the field. â€”swyx, Curator, AI.Engineer AI Engineering is a practical guide that provides the most up-to-date information on AI development, making it approachable for novice and expert leaders alike. This book is an essential resource for anyone looking to build robust and scalable AI systems. â€”Vicki Reyzelman, Chief AI Solutions Architect, Mave Sparks AI Engineering is a comprehensive guide that serves as an essential reference for both understanding and implementing AI systems in practice. â€”Han Lee, Directorâ€”Data Science, Moodyâ€™s AI Engineering is an essential guide for anyone building software with Generative AI! It demystifies the technology, highlights the importance of evaluation, and shares what should be done to achieve quality before starting with costly fine-tuning. â€”Rafal Kawala, Senior AI Engineering Director, 16 years of experience working in a Fortune 500 company',\n",
       " 'metadata': {'source': 'AI Engineering.pdf', 'page': 4, 'char_count': 1145}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b74eb728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import math\n",
    "\n",
    "\n",
    "class TextChunker:\n",
    "    \"\"\"\n",
    "    Chunk normalized documents into token-aware chunks\n",
    "    suitable for dense + sparse retrieval.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        chunk_size: int = 300,\n",
    "        overlap: int = 50\n",
    "    ):\n",
    "        if overlap >= chunk_size:\n",
    "            raise ValueError(\"overlap must be smaller than chunk_size\")\n",
    "\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap\n",
    "\n",
    "    def _estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Approximate token count.\n",
    "        (Roughly 4 chars per token for English)\n",
    "        \"\"\"\n",
    "        return math.ceil(len(text) / 4)\n",
    "\n",
    "    def _split_into_chunks(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split text into overlapping chunks.\n",
    "        \"\"\"\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "\n",
    "        start = 0\n",
    "        while start < len(words):\n",
    "            end = start + self.chunk_size\n",
    "            chunk_words = words[start:end]\n",
    "            chunks.append(\" \".join(chunk_words))\n",
    "\n",
    "            start = end - self.overlap\n",
    "            if start < 0:\n",
    "                start = 0\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def chunk_documents(self, documents: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Convert documents into chunks while preserving metadata.\n",
    "        \"\"\"\n",
    "        chunked_docs = []\n",
    "\n",
    "        for doc in documents:\n",
    "            text = doc[\"text\"]\n",
    "            base_metadata = doc[\"metadata\"]\n",
    "\n",
    "            chunks = self._split_into_chunks(text)\n",
    "\n",
    "            for idx, chunk_text in enumerate(chunks):\n",
    "                chunked_docs.append(\n",
    "                    {\n",
    "                        \"text\": chunk_text,\n",
    "                        \"metadata\": {\n",
    "                            **base_metadata,\n",
    "                            \"chunk_id\": f\"{base_metadata['source']}_p{base_metadata['page']}_c{idx}\",\n",
    "                            \"chunk_index\": idx,\n",
    "                            \"chunk_char_count\": len(chunk_text),\n",
    "                            \"chunk_token_estimate\": self._estimate_tokens(chunk_text)\n",
    "                        }\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return chunked_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80a39348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 855\n",
      "{'text': 'â€¢ Various neural network architectures, including feedforward, recurrent, and transformer. â€¢ Metrics such as accuracy, F1, precision, recall, cosine similarity, and cross entropy. If you donâ€™t know them yet, donâ€™t worryâ€”this book has either brief, high-level explanations or pointers to resources that can get you up to speed. Who This Book Is For This book is for anyone who wants to leverage foundation models to solve real-world problems. This is a technical book, so the language of this book is geared toward technical roles, including AI engineers, ML engineers, data scientists, engineering managers, and technical product managers. This book is for you if you can relate to one of the following scenarios: â€¢ Youâ€™re building or optimizing an AI application, whether youâ€™re starting from scratch or looking to move beyond the demo phase into a production-ready stage. You may also be facing issues like hallucinations, security, latency, or costs, and need targeted solutions. â€¢ You want to streamline your teamâ€™s AI development process, making it more systematic, faster, and reliable. â€¢ You want to understand how your organization can leverage foundation models to improve the businessâ€™s bottom line and how to build a team to do so. You can also benefit from the book if you belong to one of the following groups: â€¢ Tool developers who want to identify underserved areas in AI engineering to position your products in the ecosystem. â€¢ Researchers who want to better understand AI use cases. â€¢ Job candidates seeking clarity on the skills needed to pursue a career as an AI engineer. â€¢ Anyone wanting to better understand AIâ€™s capabilities and limitations, and how it might affect different roles. I love getting to the bottom of things, so some sections dive a bit deeper into the techâ€ nical side. While many early readers', 'metadata': {'source': 'AI Engineering.pdf', 'page': 17, 'char_count': 2017, 'chunk_id': 'AI Engineering.pdf_p17_c0', 'chunk_index': 0, 'chunk_char_count': 1833, 'chunk_token_estimate': 459}}\n"
     ]
    }
   ],
   "source": [
    "chunker = TextChunker(\n",
    "    chunk_size=300,   # tokens (approx)\n",
    "    overlap=50\n",
    ")\n",
    "\n",
    "chunks = chunker.chunk_documents(clean_documents)\n",
    "\n",
    "print(f\"Total chunks created: {len(chunks)}\")\n",
    "print(chunks[10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47e5b0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from qdrant_client import QdrantClient\n",
    "# from qdrant_client.models import (\n",
    "#     VectorParams,\n",
    "#     SparseVectorParams,\n",
    "#     Distance\n",
    "# )\n",
    "\n",
    "# def create_hybrid_collection(config: dict):\n",
    "#     client = QdrantClient(url=config[\"vector_store\"][\"url\"])\n",
    "\n",
    "#     collection_name = config[\"vector_store\"][\"collection_name\"]\n",
    "\n",
    "#     if client.collection_exists(collection_name):\n",
    "#         print(f\"Collection '{collection_name}' already exists.\")\n",
    "#         return\n",
    "\n",
    "#     client.create_collection(\n",
    "#         collection_name=collection_name,\n",
    "#         vectors_config={\n",
    "#             config[\"dense_vector\"][\"name\"]: VectorParams(\n",
    "#                 size=config[\"dense_vector\"][\"size\"],\n",
    "#                 distance=Distance.COSINE,\n",
    "#             )\n",
    "#         },\n",
    "#         sparse_vectors_config={\n",
    "#             config[\"sparse_vector\"][\"name\"]: SparseVectorParams()\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "#     print(f\"Hybrid collection '{collection_name}' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d279ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\mmrag\\document_search_poc\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collections=[CollectionDescription(name='knowledge_base_chunks')]\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "print(client.get_collections())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7316d54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomllib\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "class ConfigLoader:\n",
    "    def __init__(self, path: str):\n",
    "        self.path = Path(path)\n",
    "\n",
    "    def load(self) -> Dict:\n",
    "        with self.path.open(\"rb\") as f:\n",
    "            return tomllib.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35a11a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'qdrant_client.http.models.models.Document'>\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import models\n",
    "print(models.Document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3dfdd293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomllib\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "class ConfigLoader:\n",
    "    def __init__(self, path: str):\n",
    "        self.path = Path(path)\n",
    "\n",
    "    def load(self) -> Dict:\n",
    "        with self.path.open(\"rb\") as f:\n",
    "            return tomllib.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b24255d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "\n",
    "class QdrantHybridCollectionManager:\n",
    "    \"\"\"\n",
    "    Manages lifecycle of a hybrid (dense + sparse) Qdrant collection.\n",
    "    Responsible ONLY for schema, not data ingestion.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, client: QdrantClient, config: dict):\n",
    "        self.client = client\n",
    "\n",
    "        self.collection_name = config[\"vector_store\"][\"collection_name\"]\n",
    "\n",
    "        self.dense_cfg = config[\"dense_vector\"]\n",
    "        self.sparse_cfg = config[\"sparse_vector\"]\n",
    "\n",
    "    def recreate_collection(self) -> None:\n",
    "        \"\"\"\n",
    "        Delete existing collection (if any) and create a fresh hybrid collection.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.client.collection_exists(self.collection_name):\n",
    "            self.client.delete_collection(self.collection_name)\n",
    "\n",
    "        self.client.create_collection(\n",
    "            collection_name=self.collection_name,\n",
    "            vectors_config={\n",
    "                self.dense_cfg[\"name\"]: models.VectorParams(\n",
    "                    size=self._dense_vector_size(),\n",
    "                    distance=models.Distance[\n",
    "                        self.dense_cfg[\"distance\"].upper()\n",
    "                    ],\n",
    "                )\n",
    "            },\n",
    "            sparse_vectors_config={\n",
    "                self.sparse_cfg[\"name\"]: models.SparseVectorParams()\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def _dense_vector_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Resolve dense vector size using FastEmbed.\n",
    "        \"\"\"\n",
    "        return self.client.get_embedding_size(\n",
    "            self.dense_cfg[\"model\"]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e468bed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Hybrid collection created (dense + sparse)\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# Load config\n",
    "config = ConfigLoader(\"parameters.toml\").load()\n",
    "\n",
    "# Init Qdrant client\n",
    "client = QdrantClient(\n",
    "    url=config[\"vector_store\"][\"url\"]\n",
    ")\n",
    "\n",
    "# Manage collection\n",
    "collection_manager = QdrantHybridCollectionManager(\n",
    "    client=client,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "collection_manager.recreate_collection()\n",
    "\n",
    "print(\"âœ… Hybrid collection created (dense + sparse)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93f02f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from qdrant_client import models\n",
    "\n",
    "\n",
    "class HybridDocumentBuilder:\n",
    "    \"\"\"\n",
    "    Converts text chunks into FastEmbed-backed Qdrant documents\n",
    "    (dense + sparse).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dense_cfg: dict, sparse_cfg: dict):\n",
    "        self.dense_name = dense_cfg[\"name\"]\n",
    "        self.dense_model = dense_cfg[\"model\"]\n",
    "\n",
    "        self.sparse_name = sparse_cfg[\"name\"]\n",
    "        self.sparse_model = sparse_cfg[\"model\"]\n",
    "\n",
    "    def build(self, chunks: List[Dict]):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            documents: list of {vector_name: models.Document}\n",
    "            payloads:  list of metadata dicts\n",
    "        \"\"\"\n",
    "        documents = []\n",
    "        payloads = []\n",
    "\n",
    "        for chunk in chunks:\n",
    "            text = chunk[\"text\"]\n",
    "\n",
    "            documents.append(\n",
    "                {\n",
    "                    self.dense_name: models.Document(\n",
    "                        text=text,\n",
    "                        model=self.dense_model,\n",
    "                    ),\n",
    "                    self.sparse_name: models.Document(\n",
    "                        text=text,\n",
    "                        model=self.sparse_model,\n",
    "                    ),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            payloads.append(\n",
    "                {\n",
    "                    **chunk[\"metadata\"],\n",
    "                    \"text\": text,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return documents, payloads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0833f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct\n",
    "import uuid\n",
    "\n",
    "\n",
    "class QdrantIngestor:\n",
    "    \"\"\"\n",
    "    Handles ingestion of hybrid documents into Qdrant.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, client: QdrantClient, collection_name: str):\n",
    "        self.client = client\n",
    "        self.collection = collection_name\n",
    "\n",
    "    def ingest(self, documents: list, payloads: list) -> None:\n",
    "        if len(documents) != len(payloads):\n",
    "            raise ValueError(\"Documents and payloads length mismatch\")\n",
    "\n",
    "        points = []\n",
    "\n",
    "        for i in range(len(documents)):\n",
    "            vector = documents[i]\n",
    "\n",
    "            # ðŸ”’ Hard validation (prevents silent crashes)\n",
    "            if not isinstance(vector, dict):\n",
    "                raise ValueError(\n",
    "                    f\"Expected hybrid vector dict, got {type(vector)} at index {i}\"\n",
    "                )\n",
    "\n",
    "            if \"dense\" not in vector or \"sparse\" not in vector:\n",
    "                raise ValueError(\n",
    "                    f\"Hybrid vector must contain 'dense' and 'sparse' keys at index {i}\"\n",
    "                )\n",
    "\n",
    "            points.append(\n",
    "                PointStruct(\n",
    "                    id=str(uuid.uuid4()),\n",
    "                    vector={\n",
    "                        \"dense\": vector[\"dense\"],\n",
    "                        \"sparse\": vector[\"sparse\"],\n",
    "                    },\n",
    "                    payload=payloads[i],\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # âœ… SAFE, explicit ingestion\n",
    "        self.client.upsert(\n",
    "            collection_name=self.collection,\n",
    "            points=points,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f599f3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ingestion completed (dense + sparse)\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# Load config\n",
    "config = ConfigLoader(\"parameters.toml\").load()\n",
    "\n",
    "client = QdrantClient(\n",
    "    url=config[\"vector_store\"][\"url\"]\n",
    ")\n",
    "\n",
    "# Build hybrid documents\n",
    "builder = HybridDocumentBuilder(\n",
    "    dense_cfg=config[\"dense_vector\"],\n",
    "    sparse_cfg=config[\"sparse_vector\"],\n",
    ")\n",
    "\n",
    "documents, payloads = builder.build(chunks)\n",
    "\n",
    "# Ingest into Qdrant\n",
    "ingestor = QdrantIngestor(\n",
    "    client=client,\n",
    "    collection_name=config[\"vector_store\"][\"collection_name\"],\n",
    ")\n",
    "\n",
    "ingestor.ingest(documents, payloads)\n",
    "\n",
    "print(\"âœ… Ingestion completed (dense + sparse)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "575a1ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import models\n",
    "\n",
    "\n",
    "class HybridQueryBuilder:\n",
    "    \"\"\"\n",
    "    Builds dense + sparse query objects for FastEmbed-based hybrid search.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dense_cfg: dict, sparse_cfg: dict):\n",
    "        self.dense_name = dense_cfg[\"name\"]\n",
    "        self.dense_model = dense_cfg[\"model\"]\n",
    "\n",
    "        self.sparse_name = sparse_cfg[\"name\"]\n",
    "        self.sparse_model = sparse_cfg[\"model\"]\n",
    "\n",
    "    def build_prefetch(self, query: str, limit: int):\n",
    "        \"\"\"\n",
    "        Builds prefetch queries for dense + sparse retrieval.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=query,\n",
    "                    model=self.dense_model,\n",
    "                ),\n",
    "                using=self.dense_name,\n",
    "                limit=limit,\n",
    "            ),\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=query,\n",
    "                    model=self.sparse_model,\n",
    "                ),\n",
    "                using=self.sparse_name,\n",
    "                limit=limit,\n",
    "            ),\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "520b4366",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "class HybridSearcher:\n",
    "    \"\"\"\n",
    "    Executes hybrid search (Dense + Sparse + RRF) against Qdrant.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        client: QdrantClient,\n",
    "        collection_name: str,\n",
    "        query_builder: HybridQueryBuilder,\n",
    "    ):\n",
    "        self.client = client\n",
    "        self.collection = collection_name\n",
    "        self.query_builder = query_builder\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = 5,\n",
    "        prefetch_k: int = 20,\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Perform hybrid search using Reciprocal Rank Fusion (RRF).\n",
    "        \"\"\"\n",
    "\n",
    "        prefetch = self.query_builder.build_prefetch(\n",
    "            query=query,\n",
    "            limit=prefetch_k,\n",
    "        )\n",
    "\n",
    "        result = self.client.query_points(\n",
    "            collection_name=self.collection,\n",
    "            query=models.FusionQuery(\n",
    "                fusion=models.Fusion.RRF\n",
    "            ),\n",
    "            prefetch=prefetch,\n",
    "            limit=top_k,\n",
    "            with_payload=True,\n",
    "        )\n",
    "\n",
    "        return [point.payload for point in result.points]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "964c5c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': 'AI Engineering.pdf',\n",
       "  'page': 235,\n",
       "  'char_count': 1981,\n",
       "  'chunk_id': 'AI Engineering.pdf_p235_c0',\n",
       "  'chunk_index': 0,\n",
       "  'chunk_char_count': 1951,\n",
       "  'chunk_token_estimate': 488,\n",
       "  'text': '1 In its short existence, prompt engineering has managed to generate an incredible amount of animosity. Comâ€ plaints about how prompt engineering is not a real thing have gathered thousands of supporting comments; see 1, 2, 3, 4. When I told people that my upcoming book has a chapter on prompt engineering, many rolled their eyes. CHAPTER 5 Prompt Engineering Prompt engineering refers to the process of crafting an instruction that gets a model to generate the desired outcome. Prompt engineering is the easiest and most comâ€ mon model adaptation technique. Unlike finetuning, prompt engineering guides a modelâ€™s behavior without changing the modelâ€™s weights. Thanks to the strong base capabilities of foundation models, many people have successfully adapted them for applications using prompt engineering alone. You should make the most out of prompting before moving to more resource-intensive techniques like finetuning. Prompt engineeringâ€™s ease of use can mislead people into thinking that thereâ€™s not much to it.1 At first glance, prompt engineering looks like itâ€™s just fiddling with words until something works. While prompt engineering indeed involves a lot of fiddling, it also involves many interesting challenges and ingenious solutions. You can think of prompt engineering as human-to-AI communication: you communicate with AI models to get them to do what you want. Anyone can communicate, but not everyâ€ one can communicate effectively. Similarly, itâ€™s easy to write prompts but not easy to construct effective prompts. Some people argue that â€œprompt engineeringâ€ lacks the rigor to qualify as an engiâ€ neering discipline. However, this doesnâ€™t have to be the case. Prompt experiments should be conducted with the same rigor as any ML experiment, with systematic experimentation and evaluation. The importance of prompt engineering is perfectly summarized by a research manâ€ ager at OpenAI that I interviewed: â€œThe problem is not with'},\n",
       " {'source': 'AI Engineering.pdf',\n",
       "  'page': 246,\n",
       "  'char_count': 1530,\n",
       "  'chunk_id': 'AI Engineering.pdf_p246_c1',\n",
       "  'chunk_index': 1,\n",
       "  'chunk_char_count': 21,\n",
       "  'chunk_token_estimate': 6,\n",
       "  'text': '5: Prompt Engineering'},\n",
       " {'source': 'AI Engineering.pdf',\n",
       "  'page': 235,\n",
       "  'char_count': 1981,\n",
       "  'chunk_id': 'AI Engineering.pdf_p235_c0',\n",
       "  'chunk_index': 0,\n",
       "  'chunk_char_count': 1951,\n",
       "  'chunk_token_estimate': 488,\n",
       "  'text': '1 In its short existence, prompt engineering has managed to generate an incredible amount of animosity. Comâ€ plaints about how prompt engineering is not a real thing have gathered thousands of supporting comments; see 1, 2, 3, 4. When I told people that my upcoming book has a chapter on prompt engineering, many rolled their eyes. CHAPTER 5 Prompt Engineering Prompt engineering refers to the process of crafting an instruction that gets a model to generate the desired outcome. Prompt engineering is the easiest and most comâ€ mon model adaptation technique. Unlike finetuning, prompt engineering guides a modelâ€™s behavior without changing the modelâ€™s weights. Thanks to the strong base capabilities of foundation models, many people have successfully adapted them for applications using prompt engineering alone. You should make the most out of prompting before moving to more resource-intensive techniques like finetuning. Prompt engineeringâ€™s ease of use can mislead people into thinking that thereâ€™s not much to it.1 At first glance, prompt engineering looks like itâ€™s just fiddling with words until something works. While prompt engineering indeed involves a lot of fiddling, it also involves many interesting challenges and ingenious solutions. You can think of prompt engineering as human-to-AI communication: you communicate with AI models to get them to do what you want. Anyone can communicate, but not everyâ€ one can communicate effectively. Similarly, itâ€™s easy to write prompts but not easy to construct effective prompts. Some people argue that â€œprompt engineeringâ€ lacks the rigor to qualify as an engiâ€ neering discipline. However, this doesnâ€™t have to be the case. Prompt experiments should be conducted with the same rigor as any ML experiment, with systematic experimentation and evaluation. The importance of prompt engineering is perfectly summarized by a research manâ€ ager at OpenAI that I interviewed: â€œThe problem is not with'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# Load config\n",
    "config = ConfigLoader(\"parameters.toml\").load()\n",
    "\n",
    "client = QdrantClient(\n",
    "    url=config[\"vector_store\"][\"url\"]\n",
    ")\n",
    "\n",
    "# Build query components\n",
    "query_builder = HybridQueryBuilder(\n",
    "    dense_cfg=config[\"dense_vector\"],\n",
    "    sparse_cfg=config[\"sparse_vector\"],\n",
    ")\n",
    "\n",
    "searcher = HybridSearcher(\n",
    "    client=client,\n",
    "    collection_name=config[\"vector_store\"][\"collection_name\"],\n",
    "    query_builder=query_builder,\n",
    ")\n",
    "\n",
    "# Run hybrid search\n",
    "results = searcher.search(\n",
    "    query=\"What is prompt engineering?\",\n",
    "    top_k=config[\"rag\"][\"top_k\"],\n",
    ")\n",
    "\n",
    "results[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "741415bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "class RAGPromptBuilder:\n",
    "    \"\"\"\n",
    "    Builds a grounded RAG prompt from retrieved chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    SYSTEM_PROMPT = (\n",
    "        \"You are a helpful AI assistant. \"\n",
    "        \"Answer the question using ONLY the provided context. \"\n",
    "        \"If the answer is not contained in the context, say \"\n",
    "        \"'I don't know based on the provided document.'\"\n",
    "    )\n",
    "\n",
    "    def build(self, query: str, contexts: List[Dict]) -> List[Dict]:\n",
    "        context_text = \"\\n\\n\".join(\n",
    "            f\"[Context {i+1}]\\n{ctx['text']}\"\n",
    "            for i, ctx in enumerate(contexts)\n",
    "        )\n",
    "\n",
    "        user_prompt = f\"\"\"\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": self.SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "340f1002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "\n",
    "class OpenAIAnswerGenerator:\n",
    "    \"\"\"\n",
    "    Generates answers using OpenAI chat models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: str):\n",
    "        self.client = OpenAI()\n",
    "        self.model = model\n",
    "\n",
    "    @retry(\n",
    "        stop=stop_after_attempt(3),\n",
    "        wait=wait_exponential(min=2, max=10),\n",
    "    )\n",
    "    def generate(self, messages: list) -> str:\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            temperature=0.2,\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "591beba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipeline:\n",
    "    \"\"\"\n",
    "    Full RAG pipeline:\n",
    "    Query â†’ Hybrid Search â†’ OpenAI Answer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        searcher,\n",
    "        prompt_builder: RAGPromptBuilder,\n",
    "        generator: OpenAIAnswerGenerator,\n",
    "        top_k: int,\n",
    "    ):\n",
    "        self.searcher = searcher\n",
    "        self.prompt_builder = prompt_builder\n",
    "        self.generator = generator\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def answer(self, query: str) -> str:\n",
    "        contexts = self.searcher.search(\n",
    "            query=query,\n",
    "            top_k=self.top_k,\n",
    "        )\n",
    "\n",
    "        messages = self.prompt_builder.build(\n",
    "            query=query,\n",
    "            contexts=contexts,\n",
    "        )\n",
    "\n",
    "        return self.generator.generate(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0297516b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt engineering refers to the process of crafting an instruction that gets a model to generate the desired outcome. It is the easiest and most common model adaptation technique, guiding a modelâ€™s behavior without changing its weights. Prompt engineering involves human-to-AI communication, where effective prompts are constructed to achieve specific results.\n"
     ]
    }
   ],
   "source": [
    "# Load config\n",
    "config = ConfigLoader(\"parameters.toml\").load()\n",
    "\n",
    "# Build RAG components\n",
    "prompt_builder = RAGPromptBuilder()\n",
    "\n",
    "generator = OpenAIAnswerGenerator(\n",
    "    model=config[\"llm\"][\"chat_model\"]\n",
    ")\n",
    "\n",
    "rag = RAGPipeline(\n",
    "    searcher=searcher, \n",
    "    prompt_builder=prompt_builder,\n",
    "    generator=generator,\n",
    "    top_k=config[\"rag\"][\"top_k\"],\n",
    ")\n",
    "\n",
    "\n",
    "answer = rag.answer(\n",
    "    \"What is prompt engineering?\"\n",
    ")\n",
    "\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b4c724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bafd10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
