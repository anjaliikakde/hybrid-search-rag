{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a08a2fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load environment variables and validate required secrets.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    " \n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY or not OPENAI_API_KEY.strip():\n",
    "    raise EnvironmentError(\n",
    "        \"OPENAI_API_KEY not found. Please set it in the .env file.\"\n",
    "    )\n",
    "\n",
    "print(\"Environment variables loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4598e59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'llm': {'provider': 'openai', 'chat_model': 'gpt-4o-mini'},\n",
       " 'rag': {'corpus_path': '../data/AI Engineering.pdf',\n",
       "  'chunk_size': 512,\n",
       "  'chunk_overlap': 64,\n",
       "  'top_k': 5},\n",
       " 'vector_store': {'type': 'qdrant',\n",
       "  'mode': 'local',\n",
       "  'url': 'http://localhost:6333',\n",
       "  'collection_name': 'knowledge_base_chunks',\n",
       "  'storage_path': './qdrant_storage'},\n",
       " 'dense_vector': {'name': 'dense',\n",
       "  'model': 'sentence-transformers/all-MiniLM-L6-v2',\n",
       "  'distance': 'cosine'},\n",
       " 'sparse_vector': {'enabled': True, 'name': 'sparse', 'model': 'Qdrant/bm25'}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load and validate project configuration from parameters.toml.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import tomllib\n",
    "\n",
    "CONFIG_PATH = Path(\"parameters.toml\")\n",
    "\n",
    "if not CONFIG_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"parameters.toml not found. Please ensure it exists at project root.\"\n",
    "    )\n",
    "\n",
    "with open(CONFIG_PATH, \"rb\") as f:\n",
    "    config = tomllib.load(f)\n",
    "\n",
    "print(\"Configuration loaded successfully.\")\n",
    "config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56cb103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "class PDFDocumentLoader:\n",
    "    \"\"\"\n",
    "    Responsible for:\n",
    "    1. Opening a PDF safely\n",
    "    2. Counting pages\n",
    "    3. Extracting page-wise text with metadata\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pdf_path: str):\n",
    "        self.pdf_path = Path(pdf_path)\n",
    "\n",
    "        if not self.pdf_path.exists():\n",
    "            raise FileNotFoundError(f\"PDF not found at path: {self.pdf_path}\")\n",
    "\n",
    "    def get_total_pages(self) -> int:\n",
    "        \"\"\"Return total number of pages in the PDF\"\"\"\n",
    "        with fitz.open(self.pdf_path) as pdf_doc:\n",
    "            return pdf_doc.page_count\n",
    "\n",
    "    def extract_documents(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract text page-wise and return documents list\n",
    "        Each item is RAG-ready (text + metadata)\n",
    "        \"\"\"\n",
    "        documents = []\n",
    "\n",
    "        with fitz.open(self.pdf_path) as pdf_doc:\n",
    "            total_pages = pdf_doc.page_count\n",
    "\n",
    "            for page_index in range(total_pages):\n",
    "                page = pdf_doc.load_page(page_index)\n",
    "                text = page.get_text(\"text\")\n",
    "\n",
    "                # Skip empty / non-text pages\n",
    "                if not text or not text.strip():\n",
    "                    continue\n",
    "\n",
    "                documents.append(\n",
    "                    {\n",
    "                        \"text\": text.strip(),\n",
    "                        \"metadata\": {\n",
    "                            \"source\": self.pdf_path.name,\n",
    "                            \"page\": page_index + 1,\n",
    "                            \"char_count\": len(text)\n",
    "                        }\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa9b1e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages in PDF: 535\n",
      "Extracted text from 529 pages\n"
     ]
    }
   ],
   "source": [
    "loader = PDFDocumentLoader(config[\"rag\"][\"corpus_path\"])\n",
    "\n",
    "total_pages = loader.get_total_pages()\n",
    "print(f\"Total pages in PDF: {total_pages}\")\n",
    "\n",
    "documents = loader.extract_documents()\n",
    "print(f\"Extracted text from {len(documents)} pages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ef247b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "class DocumentInspector:\n",
    "    \"\"\"\n",
    "    Utility class to inspect extracted documents\n",
    "    before chunking and embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, documents: List[Dict]):\n",
    "        if not documents:\n",
    "            raise ValueError(\"No documents provided for inspection.\")\n",
    "        self.documents = documents\n",
    "\n",
    "    def preview(\n",
    "        self,\n",
    "        sample_size: int = 3,\n",
    "        max_chars: int = 500\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Print a small sample of extracted text with metadata.\n",
    "\n",
    "        :param sample_size: Number of documents to inspect\n",
    "        :param max_chars: Max characters to print per document\n",
    "        \"\"\"\n",
    "        print(f\"\\nInspecting {min(sample_size, len(self.documents))} document(s):\\n\")\n",
    "\n",
    "        for idx, doc in enumerate(self.documents[:sample_size], start=1):\n",
    "            text_preview = doc[\"text\"][:max_chars]\n",
    "\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"Sample #{idx}\")\n",
    "            print(f\"Source : {doc['metadata'].get('source')}\")\n",
    "            print(f\"Page   : {doc['metadata'].get('page')}\")\n",
    "            print(f\"Chars  : {doc['metadata'].get('char_count')}\")\n",
    "            print(\"-\" * 80)\n",
    "            print(text_preview)\n",
    "            print(\"...\" if len(doc[\"text\"]) > max_chars else \"\")\n",
    "            print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "275197e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inspecting 2 document(s):\n",
      "\n",
      "================================================================================\n",
      "Sample #1\n",
      "Source : AI Engineering.pdf\n",
      "Page   : 1\n",
      "Chars  : 73\n",
      "--------------------------------------------------------------------------------\n",
      "Chip Huyen\n",
      " AI Engineering\n",
      "Building Applications \n",
      "with Foundation Models\n",
      "\n",
      "================================================================================\n",
      "================================================================================\n",
      "Sample #2\n",
      "Source : AI Engineering.pdf\n",
      "Page   : 2\n",
      "Chars  : 2340\n",
      "--------------------------------------------------------------------------------\n",
      "9\n",
      "7 8 1 0 9 8 1 6 6 3 0 4\n",
      "5 7 9 9 9\n",
      "ISBN:   978-1-098-16630-4\n",
      "US $79.99\t   CAN $99.99\n",
      "DATA\n",
      "Foundation models have enabled many new AI use cases while lowering the barriers to entry for \n",
      "building AI products. This has transformed AI from an esoteric discipline into a powerful development \n",
      "tool that anyone can use—including those with no prior AI experience.\n",
      "In this accessible guide, author Chip Huy\n",
      "...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "inspector = DocumentInspector(documents)\n",
    "inspector.preview(sample_size=2, max_chars=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "814b5a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "class TextNormalizer:\n",
    "    \"\"\"\n",
    "    Normalize raw PDF text to improve downstream\n",
    "    chunking and retrieval quality.\n",
    "    \"\"\"\n",
    "\n",
    "    _MULTIPLE_NEWLINES = re.compile(r\"\\n{3,}\")\n",
    "    _MULTIPLE_SPACES = re.compile(r\"[ \\t]{2,}\")\n",
    "    _SPACE_BEFORE_NEWLINE = re.compile(r\"[ \\t]+\\n\")\n",
    "    _LINE_WRAP = re.compile(r\"(?<!\\n)\\n(?!\\n)\")\n",
    "\n",
    "    def normalize_text(self, text: str) -> str:\n",
    "        if not text:\n",
    "            return text\n",
    "\n",
    "        # Normalize line endings\n",
    "        text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "        # Remove trailing spaces before newlines\n",
    "        text = self._SPACE_BEFORE_NEWLINE.sub(\"\\n\", text)\n",
    "\n",
    "        # Remove line-wrapped newlines (inside paragraphs)\n",
    "        text = self._LINE_WRAP.sub(\" \", text)\n",
    "\n",
    "        # Collapse excessive newlines (keep paragraphs)\n",
    "        text = self._MULTIPLE_NEWLINES.sub(\"\\n\\n\", text)\n",
    "\n",
    "        # Collapse repeated spaces\n",
    "        text = self._MULTIPLE_SPACES.sub(\" \", text)\n",
    "\n",
    "        return text.strip()\n",
    "\n",
    "    def normalize_documents(self, documents: List[Dict]) -> List[Dict]:\n",
    "        return [\n",
    "            {\n",
    "                \"text\": self.normalize_text(doc[\"text\"]),\n",
    "                \"metadata\": doc[\"metadata\"]\n",
    "            }\n",
    "            for doc in documents\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36fe9f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '9 7 8 1 0 9 8 1 6 6 3 0 4 5 7 9 9 9 ISBN: 978-1-098-16630-4 US $79.99 CAN $99.99 DATA Foundation models have enabled many new AI use cases while lowering the barriers to entry for building AI products. This has transformed AI from an esoteric discipline into a powerful development tool that anyone can use—including those with no prior AI experience. In this accessible guide, author Chip Huyen discusses AI engineering: the process of building applications with readily available foundation models. AI application developers will discover how to navigate the AI landscape, including models, datasets, evaluation benchmarks, and the seemingly infinite number of application patterns. The book also introduces a practical framework for developing an AI application and efficiently deploying it. • Understand what AI engineering is and how it differs from traditional machine learning engineering • Learn the process for developing an AI application, the challenges at each step, and approaches to address them • Explore various model adaptation techniques, including prompt engineering, RAG, finetuning, agents, and dataset engineering, and understand how and why they work • Examine the bottlenecks for latency and cost when serving foundation models and learn how to overcome them • Choose the right model, metrics, data, and developmental patterns for your needs AI Engineering “This book of fers a comprehensive, well-structured guide to the essential aspects of building generative AI systems. A must-read for any professional looking to scale AI across the enterprise.” Vittorio Cretella, former global CIO at P&G and Mars “Chip Huyen gets generative AI. She is a remarkable teacher and writer whose work has been instrumental in helping teams bring AI into production. Drawing on her deep expertise, AI Engineering is a comprehensive and holistic guide to building generative AI applications in production.” Luke Metz, cocreator of ChatGPT, former research manager at OpenAI Chip Huyen works at the intersection of AI, data, and storytelling. Previously, she was with Snorkel AI and NVIDIA, founded an AI infrastructure startup (acquired), and taught machine learning systems design at Stanford. Her book Designing Machine Learning Systems (O’Reilly) has been translated into over 10 languages.',\n",
       " 'metadata': {'source': 'AI Engineering.pdf', 'page': 2, 'char_count': 2340}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer = TextNormalizer()\n",
    "normalized_documents = normalizer.normalize_documents(documents)\n",
    "normalized_documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cda32904",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentFilter:\n",
    "    \"\"\"\n",
    "    Filters out low-value or noisy pages\n",
    "    before chunking and embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, min_char_count: int = 200):\n",
    "        self.min_char_count = min_char_count\n",
    "\n",
    "    def is_useful(self, doc: dict) -> bool:\n",
    "        text = doc[\"text\"].lower()\n",
    "\n",
    "        # Filter very small pages\n",
    "        if doc[\"metadata\"][\"char_count\"] < self.min_char_count:\n",
    "            return False\n",
    "\n",
    "        # Filter common front-matter patterns\n",
    "        noise_markers = [\n",
    "            \"isbn\",\n",
    "            \"copyright\",\n",
    "            \"all rights reserved\",\n",
    "            \"table of contents\",\n",
    "            \"price\",\n",
    "            \"publisher\"\n",
    "        ]\n",
    "\n",
    "        if any(marker in text for marker in noise_markers):\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def filter_documents(self, documents: list) -> list:\n",
    "        return [doc for doc in documents if self.is_useful(doc)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d514e6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering: 529\n",
      "After filtering : 484\n"
     ]
    }
   ],
   "source": [
    "filterer = DocumentFilter(min_char_count=200)\n",
    "clean_documents = filterer.filter_documents(normalized_documents)\n",
    "\n",
    "print(f\"Before filtering: {len(normalized_documents)}\")\n",
    "print(f\"After filtering : {len(clean_documents)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d08432a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'This is the definitive segue into AI engineering from one of the greats of ML engineering! Chip has seen through successful projects and careers at every stage of a company and for the first time ever condensed her expertise for new AI Engineers entering the field. —swyx, Curator, AI.Engineer AI Engineering is a practical guide that provides the most up-to-date information on AI development, making it approachable for novice and expert leaders alike. This book is an essential resource for anyone looking to build robust and scalable AI systems. —Vicki Reyzelman, Chief AI Solutions Architect, Mave Sparks AI Engineering is a comprehensive guide that serves as an essential reference for both understanding and implementing AI systems in practice. —Han Lee, Director—Data Science, Moody’s AI Engineering is an essential guide for anyone building software with Generative AI! It demystifies the technology, highlights the importance of evaluation, and shares what should be done to achieve quality before starting with costly fine-tuning. —Rafal Kawala, Senior AI Engineering Director, 16 years of experience working in a Fortune 500 company',\n",
       " 'metadata': {'source': 'AI Engineering.pdf', 'page': 4, 'char_count': 1145}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b74eb728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import math\n",
    "\n",
    "\n",
    "class TextChunker:\n",
    "    \"\"\"\n",
    "    Chunk normalized documents into token-aware chunks\n",
    "    suitable for dense + sparse retrieval.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        chunk_size: int = 300,\n",
    "        overlap: int = 50\n",
    "    ):\n",
    "        if overlap >= chunk_size:\n",
    "            raise ValueError(\"overlap must be smaller than chunk_size\")\n",
    "\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap\n",
    "\n",
    "    def _estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Approximate token count.\n",
    "        (Roughly 4 chars per token for English)\n",
    "        \"\"\"\n",
    "        return math.ceil(len(text) / 4)\n",
    "\n",
    "    def _split_into_chunks(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split text into overlapping chunks.\n",
    "        \"\"\"\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "\n",
    "        start = 0\n",
    "        while start < len(words):\n",
    "            end = start + self.chunk_size\n",
    "            chunk_words = words[start:end]\n",
    "            chunks.append(\" \".join(chunk_words))\n",
    "\n",
    "            start = end - self.overlap\n",
    "            if start < 0:\n",
    "                start = 0\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def chunk_documents(self, documents: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Convert documents into chunks while preserving metadata.\n",
    "        \"\"\"\n",
    "        chunked_docs = []\n",
    "\n",
    "        for doc in documents:\n",
    "            text = doc[\"text\"]\n",
    "            base_metadata = doc[\"metadata\"]\n",
    "\n",
    "            chunks = self._split_into_chunks(text)\n",
    "\n",
    "            for idx, chunk_text in enumerate(chunks):\n",
    "                chunked_docs.append(\n",
    "                    {\n",
    "                        \"text\": chunk_text,\n",
    "                        \"metadata\": {\n",
    "                            **base_metadata,\n",
    "                            \"chunk_id\": f\"{base_metadata['source']}_p{base_metadata['page']}_c{idx}\",\n",
    "                            \"chunk_index\": idx,\n",
    "                            \"chunk_char_count\": len(chunk_text),\n",
    "                            \"chunk_token_estimate\": self._estimate_tokens(chunk_text)\n",
    "                        }\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return chunked_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80a39348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 855\n",
      "{'text': '• Various neural network architectures, including feedforward, recurrent, and transformer. • Metrics such as accuracy, F1, precision, recall, cosine similarity, and cross entropy. If you don’t know them yet, don’t worry—this book has either brief, high-level explanations or pointers to resources that can get you up to speed. Who This Book Is For This book is for anyone who wants to leverage foundation models to solve real-world problems. This is a technical book, so the language of this book is geared toward technical roles, including AI engineers, ML engineers, data scientists, engineering managers, and technical product managers. This book is for you if you can relate to one of the following scenarios: • You’re building or optimizing an AI application, whether you’re starting from scratch or looking to move beyond the demo phase into a production-ready stage. You may also be facing issues like hallucinations, security, latency, or costs, and need targeted solutions. • You want to streamline your team’s AI development process, making it more systematic, faster, and reliable. • You want to understand how your organization can leverage foundation models to improve the business’s bottom line and how to build a team to do so. You can also benefit from the book if you belong to one of the following groups: • Tool developers who want to identify underserved areas in AI engineering to position your products in the ecosystem. • Researchers who want to better understand AI use cases. • Job candidates seeking clarity on the skills needed to pursue a career as an AI engineer. • Anyone wanting to better understand AI’s capabilities and limitations, and how it might affect different roles. I love getting to the bottom of things, so some sections dive a bit deeper into the tech‐ nical side. While many early readers', 'metadata': {'source': 'AI Engineering.pdf', 'page': 17, 'char_count': 2017, 'chunk_id': 'AI Engineering.pdf_p17_c0', 'chunk_index': 0, 'chunk_char_count': 1833, 'chunk_token_estimate': 459}}\n"
     ]
    }
   ],
   "source": [
    "chunker = TextChunker(\n",
    "    chunk_size=300,   # tokens (approx)\n",
    "    overlap=50\n",
    ")\n",
    "\n",
    "chunks = chunker.chunk_documents(clean_documents)\n",
    "\n",
    "print(f\"Total chunks created: {len(chunks)}\")\n",
    "print(chunks[10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d279ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collections=[CollectionDescription(name='knowledge_base_chunks')]\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "print(client.get_collections())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7316d54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomllib\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "class ConfigLoader:\n",
    "    def __init__(self, path: str):\n",
    "        self.path = Path(path)\n",
    "\n",
    "    def load(self) -> Dict:\n",
    "        with self.path.open(\"rb\") as f:\n",
    "            return tomllib.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35a11a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'qdrant_client.http.models.models.Document'>\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import models\n",
    "print(models.Document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3dfdd293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomllib\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "class ConfigLoader:\n",
    "    def __init__(self, path: str):\n",
    "        self.path = Path(path)\n",
    "\n",
    "    def load(self) -> Dict:\n",
    "        with self.path.open(\"rb\") as f:\n",
    "            return tomllib.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b24255d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "\n",
    "class QdrantHybridCollectionManager:\n",
    "    \"\"\"\n",
    "    Manages lifecycle of a hybrid (dense + sparse) Qdrant collection.\n",
    "    Responsible ONLY for schema, not data ingestion.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, client: QdrantClient, config: dict):\n",
    "        self.client = client\n",
    "\n",
    "        self.collection_name = config[\"vector_store\"][\"collection_name\"]\n",
    "\n",
    "        self.dense_cfg = config[\"dense_vector\"]\n",
    "        self.sparse_cfg = config[\"sparse_vector\"]\n",
    "\n",
    "    def recreate_collection(self) -> None:\n",
    "        \"\"\"\n",
    "        Delete existing collection (if any) and create a fresh hybrid collection.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.client.collection_exists(self.collection_name):\n",
    "            self.client.delete_collection(self.collection_name)\n",
    "\n",
    "        self.client.create_collection(\n",
    "            collection_name=self.collection_name,\n",
    "            vectors_config={\n",
    "                self.dense_cfg[\"name\"]: models.VectorParams(\n",
    "                    size=self._dense_vector_size(),\n",
    "                    distance=models.Distance[\n",
    "                        self.dense_cfg[\"distance\"].upper()\n",
    "                    ],\n",
    "                )\n",
    "            },\n",
    "            sparse_vectors_config={\n",
    "                self.sparse_cfg[\"name\"]: models.SparseVectorParams()\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def _dense_vector_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Resolve dense vector size using FastEmbed.\n",
    "        \"\"\"\n",
    "        return self.client.get_embedding_size(\n",
    "            self.dense_cfg[\"model\"]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e468bed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid collection created (dense + sparse)\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# Load config\n",
    "config = ConfigLoader(\"parameters.toml\").load()\n",
    "\n",
    "# Init Qdrant client\n",
    "client = QdrantClient(\n",
    "    url=config[\"vector_store\"][\"url\"]\n",
    ")\n",
    "\n",
    "# Manage collection\n",
    "collection_manager = QdrantHybridCollectionManager(\n",
    "    client=client,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "collection_manager.recreate_collection()\n",
    "\n",
    "print(\"Hybrid collection created (dense + sparse)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93f02f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from qdrant_client import models\n",
    "\n",
    "\n",
    "class HybridDocumentBuilder:\n",
    "    \"\"\"\n",
    "    Converts text chunks into FastEmbed-backed Qdrant documents\n",
    "    (dense + sparse).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dense_cfg: dict, sparse_cfg: dict):\n",
    "        self.dense_name = dense_cfg[\"name\"]\n",
    "        self.dense_model = dense_cfg[\"model\"]\n",
    "\n",
    "        self.sparse_name = sparse_cfg[\"name\"]\n",
    "        self.sparse_model = sparse_cfg[\"model\"]\n",
    "\n",
    "    def build(self, chunks: List[Dict]):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            documents: list of {vector_name: models.Document}\n",
    "            payloads:  list of metadata dicts\n",
    "        \"\"\"\n",
    "        documents = []\n",
    "        payloads = []\n",
    "\n",
    "        for chunk in chunks:\n",
    "            text = chunk[\"text\"]\n",
    "\n",
    "            documents.append(\n",
    "                {\n",
    "                    self.dense_name: models.Document(\n",
    "                        text=text,\n",
    "                        model=self.dense_model,\n",
    "                    ),\n",
    "                    self.sparse_name: models.Document(\n",
    "                        text=text,\n",
    "                        model=self.sparse_model,\n",
    "                    ),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            payloads.append(\n",
    "                {\n",
    "                    **chunk[\"metadata\"],\n",
    "                    \"text\": text,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return documents, payloads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0833f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct\n",
    "import uuid\n",
    "\n",
    "\n",
    "class QdrantIngestor:\n",
    "    \"\"\"\n",
    "    Handles ingestion of hybrid documents into Qdrant.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, client: QdrantClient, collection_name: str):\n",
    "        self.client = client\n",
    "        self.collection = collection_name\n",
    "\n",
    "    def ingest(self, documents: list, payloads: list) -> None:\n",
    "        if len(documents) != len(payloads):\n",
    "            raise ValueError(\"Documents and payloads length mismatch\")\n",
    "\n",
    "        points = []\n",
    "\n",
    "        for i in range(len(documents)):\n",
    "            vector = documents[i]\n",
    "\n",
    "            # validation\n",
    "            if not isinstance(vector, dict):\n",
    "                raise ValueError(\n",
    "                    f\"Expected hybrid vector dict, got {type(vector)} at index {i}\"\n",
    "                )\n",
    "\n",
    "            if \"dense\" not in vector or \"sparse\" not in vector:\n",
    "                raise ValueError(\n",
    "                    f\"Hybrid vector must contain 'dense' and 'sparse' keys at index {i}\"\n",
    "                )\n",
    "\n",
    "            points.append(\n",
    "                PointStruct(\n",
    "                    id=str(uuid.uuid4()),\n",
    "                    vector={\n",
    "                        \"dense\": vector[\"dense\"],\n",
    "                        \"sparse\": vector[\"sparse\"],\n",
    "                    },\n",
    "                    payload=payloads[i],\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # SAFE, explicit ingestion\n",
    "        self.client.upsert(\n",
    "            collection_name=self.collection,\n",
    "            points=points,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f599f3b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseHandlingException",
     "evalue": "timed out",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\mmrag\\document_search_poc\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\mmrag\\document_search_poc\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\mmrag\\document_search_poc\\venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\mmrag\\document_search_poc\\venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\mmrag\\document_search_poc\\venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\mmrag\\document_search_poc\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\mmrag\\document_search_poc\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\mmrag\\document_search_poc\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\mmrag\\document_search_poc\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\mmrag\\document_search_poc\\venv\\Lib\\site-packages\\httpcore\\_backends\\sync.py:126\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    125\u001b[39m exc_map: ExceptionMapping = {socket.timeout: ReadTimeout, \u001b[38;5;167;01mOSError\u001b[39;00m: ReadError}\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python313\\Lib\\contextlib.py:162\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\mmrag\\document_search_poc\\venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mReadTimeout\u001b[39m: timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\mmrag\\document_search_poc\\venv\\Lib\\site-packages\\qdrant_client\\http\\api_client.py:134\u001b[39m, in \u001b[36mApiClient.send_inner\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\mmrag\\document_search_poc\\venv\\Lib\\site-packages\\httpx\\_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\mmrag\\document_search_poc\\venv\\Lib\\site-packages\\httpx\\_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\mmrag\\document_search_poc\\venv\\Lib\\site-packages\\httpx\\_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\mmrag\\document_search_poc\\venv\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\mmrag\\document_search_poc\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py:249\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m    250\u001b[39m     resp = \u001b[38;5;28mself\u001b[39m._pool.handle_request(req)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python313\\Lib\\contextlib.py:162\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\mmrag\\document_search_poc\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mReadTimeout\u001b[39m: timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mResponseHandlingException\u001b[39m                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Ingest into Qdrant\u001b[39;00m\n\u001b[32m     19\u001b[39m ingestor = QdrantIngestor(\n\u001b[32m     20\u001b[39m     client=client,\n\u001b[32m     21\u001b[39m     collection_name=config[\u001b[33m\"\u001b[39m\u001b[33mvector_store\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcollection_name\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     22\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mingestor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mingest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpayloads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mIngestion completed (dense + sparse)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mQdrantIngestor.ingest\u001b[39m\u001b[34m(self, documents, payloads)\u001b[39m\n\u001b[32m     35\u001b[39m     points.append(\n\u001b[32m     36\u001b[39m         PointStruct(\n\u001b[32m     37\u001b[39m             \u001b[38;5;28mid\u001b[39m=\u001b[38;5;28mstr\u001b[39m(uuid.uuid4()),\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m         )\n\u001b[32m     44\u001b[39m     )\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# SAFE, explicit ingestion\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\mmrag\\document_search_poc\\venv\\Lib\\site-packages\\qdrant_client\\qdrant_client.py:938\u001b[39m, in \u001b[36mQdrantClient.upsert\u001b[39m\u001b[34m(self, collection_name, points, wait, ordering, shard_key_selector, update_filter, **kwargs)\u001b[39m\n\u001b[32m    931\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    932\u001b[39m         points = \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m    933\u001b[39m             \u001b[38;5;28mself\u001b[39m._embed_models(\n\u001b[32m    934\u001b[39m                 points, is_query=\u001b[38;5;28;01mFalse\u001b[39;00m, batch_size=\u001b[38;5;28mself\u001b[39m.local_inference_batch_size\n\u001b[32m    935\u001b[39m             )\n\u001b[32m    936\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m938\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43mordering\u001b[49m\u001b[43m=\u001b[49m\u001b[43mordering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshard_key_selector\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshard_key_selector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mupdate_filter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mupdate_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\mmrag\\document_search_poc\\venv\\Lib\\site-packages\\qdrant_client\\qdrant_remote.py:1121\u001b[39m, in \u001b[36mQdrantRemote.upsert\u001b[39m\u001b[34m(self, collection_name, points, wait, ordering, shard_key_selector, update_filter, **kwargs)\u001b[39m\n\u001b[32m   1116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(points, models.Batch):\n\u001b[32m   1117\u001b[39m     points = models.PointsBatch(\n\u001b[32m   1118\u001b[39m         batch=points, shard_key=shard_key_selector, update_filter=update_filter\n\u001b[32m   1119\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m http_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mopenapi_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoints_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupsert_points\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpoint_insert_operations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mordering\u001b[49m\u001b[43m=\u001b[49m\u001b[43mordering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.result\n\u001b[32m   1127\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m http_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mUpsert returned None result\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1128\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m http_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\mmrag\\document_search_poc\\venv\\Lib\\site-packages\\qdrant_client\\http\\api\\points_api.py:994\u001b[39m, in \u001b[36mSyncPointsApi.upsert_points\u001b[39m\u001b[34m(self, collection_name, wait, ordering, point_insert_operations)\u001b[39m\n\u001b[32m    984\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupsert_points\u001b[39m(\n\u001b[32m    985\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    986\u001b[39m     collection_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    989\u001b[39m     point_insert_operations: m.PointInsertOperations = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    990\u001b[39m ) -> m.InlineResponse2005:\n\u001b[32m    991\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    992\u001b[39m \u001b[33;03m    Perform insert + updates on points. If point with given ID already exists - it will be overwritten.\u001b[39;00m\n\u001b[32m    993\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m994\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_for_upsert_points\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    995\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    996\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    997\u001b[39m \u001b[43m        \u001b[49m\u001b[43mordering\u001b[49m\u001b[43m=\u001b[49m\u001b[43mordering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpoint_insert_operations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpoint_insert_operations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\mmrag\\document_search_poc\\venv\\Lib\\site-packages\\qdrant_client\\http\\api\\points_api.py:515\u001b[39m, in \u001b[36m_PointsApi._build_for_upsert_points\u001b[39m\u001b[34m(self, collection_name, wait, ordering, point_insert_operations)\u001b[39m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m headers:\n\u001b[32m    514\u001b[39m     headers[\u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtype_\u001b[49m\u001b[43m=\u001b[49m\u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mInlineResponse2005\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPUT\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/collections/\u001b[39;49m\u001b[38;5;132;43;01m{collection_name}\u001b[39;49;00m\u001b[33;43m/points\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\mmrag\\document_search_poc\\venv\\Lib\\site-packages\\qdrant_client\\http\\api_client.py:95\u001b[39m, in \u001b[36mApiClient.request\u001b[39m\u001b[34m(self, type_, method, url, path_params, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mint\u001b[39m(kwargs[\u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     94\u001b[39m request = \u001b[38;5;28mself\u001b[39m._client.build_request(method, url, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\mmrag\\document_search_poc\\venv\\Lib\\site-packages\\qdrant_client\\http\\api_client.py:112\u001b[39m, in \u001b[36mApiClient.send\u001b[39m\u001b[34m(self, request, type_)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: Request, type_: Type[T]) -> T:\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmiddleware\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend_inner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response.status_code == \u001b[32m429\u001b[39m:\n\u001b[32m    115\u001b[39m         retry_after_s = response.headers.get(\u001b[33m\"\u001b[39m\u001b[33mRetry-After\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\mmrag\\document_search_poc\\venv\\Lib\\site-packages\\qdrant_client\\http\\api_client.py:250\u001b[39m, in \u001b[36mBaseMiddleware.__call__\u001b[39m\u001b[34m(self, request, call_next)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: Request, call_next: Send) -> Response:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\mmrag\\document_search_poc\\venv\\Lib\\site-packages\\qdrant_client\\http\\api_client.py:136\u001b[39m, in \u001b[36mApiClient.send_inner\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     response = \u001b[38;5;28mself\u001b[39m._client.send(request)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResponseHandlingException(e)\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[31mResponseHandlingException\u001b[39m: timed out"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# Load config\n",
    "config = ConfigLoader(\"parameters.toml\").load()\n",
    "\n",
    "client = QdrantClient(\n",
    "    url=config[\"vector_store\"][\"url\"]\n",
    ")\n",
    "\n",
    "# Build hybrid documents\n",
    "builder = HybridDocumentBuilder(\n",
    "    dense_cfg=config[\"dense_vector\"],\n",
    "    sparse_cfg=config[\"sparse_vector\"],\n",
    ")\n",
    "\n",
    "documents, payloads = builder.build(chunks)\n",
    "\n",
    "# Ingest into Qdrant\n",
    "ingestor = QdrantIngestor(\n",
    "    client=client,\n",
    "    collection_name=config[\"vector_store\"][\"collection_name\"],\n",
    ")\n",
    "\n",
    "ingestor.ingest(documents, payloads)\n",
    "\n",
    "print(\"Ingestion completed (dense + sparse)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575a1ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import models\n",
    "\n",
    "\n",
    "class HybridQueryBuilder:\n",
    "    \"\"\"\n",
    "    Builds dense + sparse query objects for FastEmbed-based hybrid search.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dense_cfg: dict, sparse_cfg: dict):\n",
    "        self.dense_name = dense_cfg[\"name\"]\n",
    "        self.dense_model = dense_cfg[\"model\"]\n",
    "\n",
    "        self.sparse_name = sparse_cfg[\"name\"]\n",
    "        self.sparse_model = sparse_cfg[\"model\"]\n",
    "\n",
    "    def build_prefetch(self, query: str, limit: int):\n",
    "        \"\"\"\n",
    "        Builds prefetch queries for dense + sparse retrieval.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=query,\n",
    "                    model=self.dense_model,\n",
    "                ),\n",
    "                using=self.dense_name,\n",
    "                limit=limit,\n",
    "            ),\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=query,\n",
    "                    model=self.sparse_model,\n",
    "                ),\n",
    "                using=self.sparse_name,\n",
    "                limit=limit,\n",
    "            ),\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520b4366",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "class HybridSearcher:\n",
    "    \"\"\"\n",
    "    Executes hybrid search (Dense + Sparse + RRF) against Qdrant.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        client: QdrantClient,\n",
    "        collection_name: str,\n",
    "        query_builder: HybridQueryBuilder,\n",
    "    ):\n",
    "        self.client = client\n",
    "        self.collection = collection_name\n",
    "        self.query_builder = query_builder\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = 5,\n",
    "        prefetch_k: int = 20,\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Perform hybrid search using Reciprocal Rank Fusion (RRF).\n",
    "        \"\"\"\n",
    "\n",
    "        prefetch = self.query_builder.build_prefetch(\n",
    "            query=query,\n",
    "            limit=prefetch_k,\n",
    "        )\n",
    "\n",
    "        result = self.client.query_points(\n",
    "            collection_name=self.collection,\n",
    "            query=models.FusionQuery(\n",
    "                fusion=models.Fusion.RRF\n",
    "            ),\n",
    "            prefetch=prefetch,\n",
    "            limit=top_k,\n",
    "            with_payload=True,\n",
    "        )\n",
    "\n",
    "        return [point.payload for point in result.points]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964c5c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': 'AI Engineering.pdf',\n",
       "  'page': 235,\n",
       "  'char_count': 1981,\n",
       "  'chunk_id': 'AI Engineering.pdf_p235_c0',\n",
       "  'chunk_index': 0,\n",
       "  'chunk_char_count': 1951,\n",
       "  'chunk_token_estimate': 488,\n",
       "  'text': '1 In its short existence, prompt engineering has managed to generate an incredible amount of animosity. Com‐ plaints about how prompt engineering is not a real thing have gathered thousands of supporting comments; see 1, 2, 3, 4. When I told people that my upcoming book has a chapter on prompt engineering, many rolled their eyes. CHAPTER 5 Prompt Engineering Prompt engineering refers to the process of crafting an instruction that gets a model to generate the desired outcome. Prompt engineering is the easiest and most com‐ mon model adaptation technique. Unlike finetuning, prompt engineering guides a model’s behavior without changing the model’s weights. Thanks to the strong base capabilities of foundation models, many people have successfully adapted them for applications using prompt engineering alone. You should make the most out of prompting before moving to more resource-intensive techniques like finetuning. Prompt engineering’s ease of use can mislead people into thinking that there’s not much to it.1 At first glance, prompt engineering looks like it’s just fiddling with words until something works. While prompt engineering indeed involves a lot of fiddling, it also involves many interesting challenges and ingenious solutions. You can think of prompt engineering as human-to-AI communication: you communicate with AI models to get them to do what you want. Anyone can communicate, but not every‐ one can communicate effectively. Similarly, it’s easy to write prompts but not easy to construct effective prompts. Some people argue that “prompt engineering” lacks the rigor to qualify as an engi‐ neering discipline. However, this doesn’t have to be the case. Prompt experiments should be conducted with the same rigor as any ML experiment, with systematic experimentation and evaluation. The importance of prompt engineering is perfectly summarized by a research man‐ ager at OpenAI that I interviewed: “The problem is not with'},\n",
       " {'source': 'AI Engineering.pdf',\n",
       "  'page': 246,\n",
       "  'char_count': 1530,\n",
       "  'chunk_id': 'AI Engineering.pdf_p246_c1',\n",
       "  'chunk_index': 1,\n",
       "  'chunk_char_count': 21,\n",
       "  'chunk_token_estimate': 6,\n",
       "  'text': '5: Prompt Engineering'},\n",
       " {'source': 'AI Engineering.pdf',\n",
       "  'page': 244,\n",
       "  'char_count': 2483,\n",
       "  'chunk_id': 'AI Engineering.pdf_p244_c0',\n",
       "  'chunk_index': 0,\n",
       "  'chunk_char_count': 1879,\n",
       "  'chunk_token_estimate': 470,\n",
       "  'text': 'Similar tests, such as RULER (Hsieh et al., 2024), can also be used to evaluate how good a model is at processing long prompts. If the model’s performance grows increasingly worse with a longer context, then perhaps you should find a way to shorten your prompts. System prompt, user prompt, examples, and context are the key components of a prompt. Now that we’ve discussed what a prompt is and why prompting works, let’s discuss the best practices for writing effective prompts. Prompt Engineering Best Practices Prompt engineering can get incredibly hacky, especially for weaker models. In the early days of prompt engineering, many guides came out with tips such as writing “Q:” instead of “Questions:” or encouraging models to respond better with the promise of a “$300 tip for the right answer”. While these tips can be useful for some models, they can become outdated as models get better at following instructions and more robust to prompt perturbations. This section focuses on general techniques that have been proven to work with a wide range of models and will likely remain relevant in the near future. They are distilled from prompt engineering tutorials created by model providers, including OpenAI, Anthropic, Meta, and Google, and best practices shared by teams that have success‐ fully deployed generative AI applications. These companies also often provide libra‐ ries of pre-crafted prompts that you can reference—see Anthropic, Google, and OpenAI. Outside of these general practices, each model likely has its own quirks that respond to specific prompt tricks. When working with a model, you should look for prompt engineering guides specific to it. Write Clear and Explicit Instructions Communicating with AI is the same as communicating with humans: clarity helps. Here are a few tips on how to write clear instructions. Explain, without ambiguity, what you'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# Load config\n",
    "config = ConfigLoader(\"parameters.toml\").load()\n",
    "\n",
    "client = QdrantClient(\n",
    "    url=config[\"vector_store\"][\"url\"]\n",
    ")\n",
    "\n",
    "# Build query components\n",
    "query_builder = HybridQueryBuilder(\n",
    "    dense_cfg=config[\"dense_vector\"],\n",
    "    sparse_cfg=config[\"sparse_vector\"],\n",
    ")\n",
    "\n",
    "searcher = HybridSearcher(\n",
    "    client=client,\n",
    "    collection_name=config[\"vector_store\"][\"collection_name\"],\n",
    "    query_builder=query_builder,\n",
    ")\n",
    "\n",
    "# Run hybrid search\n",
    "results = searcher.search(\n",
    "    query=\"What is prompt engineering?\",\n",
    "    top_k=config[\"rag\"][\"top_k\"],\n",
    ")\n",
    "\n",
    "results[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741415bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "class RAGPromptBuilder:\n",
    "    \"\"\"\n",
    "    Builds a grounded RAG prompt from retrieved chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    SYSTEM_PROMPT = (\n",
    "        \"You are a helpful AI assistant. \"\n",
    "        \"Answer the question using ONLY the provided context. \"\n",
    "        \"If the answer is not contained in the context, say \"\n",
    "        \"'I don't know based on the provided document.'\"\n",
    "    )\n",
    "\n",
    "    def build(self, query: str, contexts: List[Dict]) -> List[Dict]:\n",
    "        context_text = \"\\n\\n\".join(\n",
    "            f\"[Context {i+1}]\\n{ctx['text']}\"\n",
    "            for i, ctx in enumerate(contexts)\n",
    "        )\n",
    "\n",
    "        user_prompt = f\"\"\"\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": self.SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340f1002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "\n",
    "class OpenAIAnswerGenerator:\n",
    "    \"\"\"\n",
    "    Generates answers using OpenAI chat models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: str):\n",
    "        self.client = OpenAI()\n",
    "        self.model = model\n",
    "\n",
    "    @retry(\n",
    "        stop=stop_after_attempt(3),\n",
    "        wait=wait_exponential(min=2, max=10),\n",
    "    )\n",
    "    def generate(self, messages: list) -> str:\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            temperature=0.2,\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591beba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipeline:\n",
    "    \"\"\"\n",
    "    Full RAG pipeline:\n",
    "    Query → Hybrid Search → OpenAI Answer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        searcher,\n",
    "        prompt_builder: RAGPromptBuilder,\n",
    "        generator: OpenAIAnswerGenerator,\n",
    "        top_k: int,\n",
    "    ):\n",
    "        self.searcher = searcher\n",
    "        self.prompt_builder = prompt_builder\n",
    "        self.generator = generator\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def answer(self, query: str) -> str:\n",
    "        contexts = self.searcher.search(\n",
    "            query=query,\n",
    "            top_k=self.top_k,\n",
    "        )\n",
    "\n",
    "        messages = self.prompt_builder.build(\n",
    "            query=query,\n",
    "            contexts=contexts,\n",
    "        )\n",
    "\n",
    "        return self.generator.generate(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0297516b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A reward model takes in a (prompt, response) pair and scores how good the response is given the prompt. It outputs a score indicating the quality of the response, and has been successfully used in reinforcement learning from human feedback (RLHF) for many years.\n"
     ]
    }
   ],
   "source": [
    "# Load config\n",
    "config = ConfigLoader(\"parameters.toml\").load()\n",
    "\n",
    "# Build RAG components\n",
    "prompt_builder = RAGPromptBuilder()\n",
    "\n",
    "generator = OpenAIAnswerGenerator(\n",
    "    model=config[\"llm\"][\"chat_model\"]\n",
    ")\n",
    "\n",
    "rag = RAGPipeline(\n",
    "    searcher=searcher, \n",
    "    prompt_builder=prompt_builder,\n",
    "    generator=generator,\n",
    "    top_k=config[\"rag\"][\"top_k\"],\n",
    ")\n",
    "\n",
    "\n",
    "answer = rag.answer(\n",
    "    \"What is reward model?\"\n",
    ")\n",
    "\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b4c724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An agent is anything that can perceive its environment through sensors and act upon that environment through actuators. It is characterized by the environment it operates in and the set of actions it can perform.\n"
     ]
    }
   ],
   "source": [
    "answer = rag.answer(\n",
    "    \"what is agent\"\n",
    ")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bafd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24262b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "210856e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from statistics import mean\n",
    "import pandas as pd\n",
    "from qdrant_client import QdrantClient, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eae3ecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "COLLECTION = \"knowledge_base_chunks\"\n",
    "TOP_K = 5\n",
    "RELEVANCE_THRESHOLD = 0.6\n",
    "\n",
    "\n",
    "queries = [\n",
    "    \"What is prompt engineering?\",\n",
    "    \"Explain vector databases\",\n",
    "    \"What is retrieval augmented generation?\",\n",
    "    \"Why hybrid search is better?\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba0e0c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_only(query: str):\n",
    "    return client.query_points(\n",
    "        collection_name=COLLECTION,\n",
    "        query=models.Document(\n",
    "            text=query,\n",
    "            model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        ),\n",
    "        using=\"dense\",\n",
    "        limit=TOP_K,\n",
    "    ).points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e923ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_only(query: str):\n",
    "    return client.query_points(\n",
    "        collection_name=COLLECTION,\n",
    "        query=models.Document(\n",
    "            text=query,\n",
    "            model=\"Qdrant/bm25\",\n",
    "        ),\n",
    "        using=\"sparse\",\n",
    "        limit=TOP_K,\n",
    "    ).points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "afc5afd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid(query: str):\n",
    "    return client.query_points(\n",
    "        collection_name=COLLECTION,\n",
    "        query=models.FusionQuery(\n",
    "            fusion=models.Fusion.RRF\n",
    "        ),\n",
    "        prefetch=[\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=query,\n",
    "                    model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                ),\n",
    "                using=\"dense\",\n",
    "                limit=20,\n",
    "            ),\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=query,\n",
    "                    model=\"Qdrant/bm25\",\n",
    "                ),\n",
    "                using=\"sparse\",\n",
    "                limit=20,\n",
    "            ),\n",
    "        ],\n",
    "        limit=TOP_K,\n",
    "    ).points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e68dd5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(name, retriever_fn):\n",
    "    latencies = []\n",
    "    avg_scores = []\n",
    "    max_scores = []\n",
    "    relevant_ratios = []\n",
    "\n",
    "    for q in queries:\n",
    "        start = time.perf_counter()\n",
    "        results = retriever_fn(q)\n",
    "        latency = (time.perf_counter() - start) * 1000\n",
    "        latencies.append(latency)\n",
    "\n",
    "        scores = [p.score for p in results if p.score is not None]\n",
    "\n",
    "        if scores:\n",
    "            avg_scores.append(mean(scores))\n",
    "            max_scores.append(max(scores))\n",
    "            relevant_ratios.append(\n",
    "                sum(s >= RELEVANCE_THRESHOLD for s in scores) / len(scores)\n",
    "            )\n",
    "        else:\n",
    "            avg_scores.append(0)\n",
    "            max_scores.append(0)\n",
    "            relevant_ratios.append(0)\n",
    "\n",
    "    return {\n",
    "        \"retriever\": name,\n",
    "        \"avg_latency_ms\": round(mean(latencies), 2),\n",
    "        \"avg_similarity_score\": round(mean(avg_scores), 4),\n",
    "        \"max_similarity_score\": round(mean(max_scores), 4),\n",
    "        \"relevant_ratio\": round(mean(relevant_ratios), 2),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1fa3ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "results.append(evaluate(\"Dense Only\", dense_only))\n",
    "results.append(evaluate(\"Sparse Only\", sparse_only))\n",
    "results.append(evaluate(\"Hybrid (RRF)\", hybrid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f359d92",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "The similarity scores shown below are NOT directly comparable across retrievers.\n",
    "\n",
    "Dense retrievers use cosine similarity (bounded between 0 and 1).\n",
    "Sparse retrievers (BM25 / SPLADE) return relevance scores based on term statistics and are unbounded.\n",
    "Hybrid retrievers use rank fusion (RRF), where the final score is a fusion score and does not represent semantic similarity.\n",
    "\n",
    "Therefore, absolute score values should not be compared across retrieval strategies.\n",
    "Evaluation should be interpreted in terms of ranking quality and latency, not raw scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fe88a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retriever</th>\n",
       "      <th>avg_latency_ms</th>\n",
       "      <th>avg_similarity_score</th>\n",
       "      <th>max_similarity_score</th>\n",
       "      <th>relevant_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dense Only</td>\n",
       "      <td>921.99</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6565</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sparse Only</td>\n",
       "      <td>307.88</td>\n",
       "      <td>3.8873</td>\n",
       "      <td>4.1842</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hybrid (RRF)</td>\n",
       "      <td>38.23</td>\n",
       "      <td>0.4796</td>\n",
       "      <td>0.7917</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      retriever  avg_latency_ms  avg_similarity_score  max_similarity_score  \\\n",
       "0    Dense Only          921.99                0.5598                0.6565   \n",
       "1   Sparse Only          307.88                3.8873                4.1842   \n",
       "2  Hybrid (RRF)           38.23                0.4796                0.7917   \n",
       "\n",
       "   relevant_ratio  \n",
       "0            0.35  \n",
       "1            1.00  \n",
       "2            0.20  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a48c55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
